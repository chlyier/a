<p align = "center" style="font-size: 30px; font-weight:bold">针对本草纲目毽子舞的AI健身教练</p>

2019211341 蔡浩龙

# 项目背景

最近，疫情的大肆横行使得人们大多居家生活工作，在这样的情况下，刘畊宏的燃脂健身直播刷屏网络，掀起了一阵狂热不退的居家健身潮，以周杰伦的《本草纲目》为旋律的毽子舞，更是引发了全民打卡热。

然而，虽然大家对于健身的热度很高，但是并非每个人都拥有一个专业的健身教练。很多人或许只是跟着视频锻炼，动作是否正确到位、效果是否达到预期都不得而知，也正是因此，才有了刘教练在线讲解动作要点以及批改网友健身作业的情况。但是这并不能帮助到每一个人，因此我根据刘教练的动作要点讲解视频，使用基于2D骨骼关键点检测的人体姿态识别技术，设计并实现了一个针对本草纲目毽子舞的AI健身教练，能够对前三个动作的要点进行实时检测指导，提醒用户动作到位，帮助大家对自己的毽子舞动作有更为精准的把控，成为更好的刘畊宏男孩女孩们。

# 技术调研

## 基于CV的人机交互

基于计算机视觉的人机交互一般可分为四个部分：

1. 有没有人；
2. 人在哪；

3. 这个人是谁；

4. 这个人在做什么；

人机交互过程中涉及的关键技术点如下：

<img src="https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529114955293.png" alt="image-20220529114955293 " style="zoom:100%;" />

在四个部分的实现中，人脸识别可以完成1、2、3部分的实现，但是第4部分的这个人在做什么，人脸识别就失效了。还有第3部分的这个人是谁，当图片的分辨率太小以至于人脸识别失效，或者图片中的人只有背景的时候，人脸识别就没法识别出这个人是谁了：

![image-20220529104758404](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529104758404.png)

如上图，人脸识别的方式不能识别出这个人是谁！这时就得用到人脸识别之外的另一个识别方法：姿态识别。

## 姿态识别简介

人体姿态是人体重要的生物特征之一，有很多的应用场景，如：步态分析、视频监控、增强现实、人机交互、金融、移动支付、娱乐和游戏、体育科学等。姿态识别能让计算机知道人在做什么、识别出这个人是谁。特别是在监控领域、在摄像头获取到的人脸图像分辨率过小的情况下是一个很好的解决方案，还有在目标身份识别系统中可以作为一项重要的辅助验证手段，达到减小误识别的效果。

**姿态识别一般流程如下：**

![image-20220529104919092](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529104919092.png)

### 人体分割

人体分割使用的方法可以大体分为人体骨骼关键点检测、语义分割等方式实现。这里主要分析与姿态相关的人体骨骼关键点检测。人体骨骼关键点检测输出是人体的骨架信息，一般主要作为人体姿态识别的基础部分，主要用于分割、对齐等。一般实现流程为：

![image-20220529105020249](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529105020249.png)

人体骨骼关键点检测也称为姿态估计（Pose Estimation），主要检测人体的关键点信息，如关节，五官等，通过关键点描述人体骨骼信息，常用来作为姿态识别、行为分析等的基础部件，如下图所示：

![image-20220529105343352](C:/Users/cc188/AppData/Roaming/Typora/typora-user-images/image-20220529105343352.png)

人体骨骼关键点检测是一种多方面任务，包含了目标检测、人体骨骼关键点检测、分割等。人体骨骼关键点检测根据输出的关键点数据是2D还是3D的，可以分为二维（2D）和三维（3D）的人体骨骼关键点检测；按照检测的方法又能分为自底向上、自顶向下两个方式。

**人体骨骼关键点检测的挑战：**

1. 每张图片中包含人的数量是未知的，图像中人越多，计算复杂度越大（计算量与人的数量正相关），这使得处理时间变长，从而使real time变得困难。

2. 人与人之间会存在如接触、遮挡等关系，导致将不同人的关键节点区分出来的难度增加，有可能会将骨骼关键点误认为是另一个人的。

3. 关键点区域的图像信息比较难区分，也就是说某个关键点检测时容易出现检测位置不准或者置信度不准，甚至将背景的图像当成关键点图像的错误。

4. 人体不同关键点检测的难易程度是不一样的，对于腰部、腿部这类没有比较明显特征关键点的检测要难于头部附近关键点的检测，需要对不同的关键点区别对待。

#### 2D人体关键点检测

（1）CPM：Convolutional Pose Machines

（2）Hourglass：Stacked Hourglass Networks for Human Pose Estimation

（3）OpenPose：Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields

（4）CPN：Cascaded Pyramid Network for Multi-Person Pose Estimation

（5）MSPN：Rethinking on Multi-Stage Networks for Human Pose Estimation

（6）HRNet：Deep High-Resolution Representation Learning for Human Pose Estimation

#### 3D人体关键点检测

（1）VideoPose3D：3D human pose estimation in video with temporal convolutions and semi-supervised training

（2）Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach

（3）PoseDRL：Deep Reinforcement Learning for Active Human Pose Estimation

#### 人体骨骼关键点检测之Mediapipe

MediaPipe Pose 是一种用于高保真身体姿势跟踪的 ML 解决方案，利用 BlazePose 研究从 RGB 视频帧推断整个身体上的 33 个 3D 地标和背景分割掩码，该研究也为 ML Kit 姿势检测 API 提供支持。当前最先进的方法主要依赖于强大的桌面环境进行推理，而该方法在大多数现代手机、台式机/笔记本电脑、python 甚至网络上实现了实时性能。

检测基于BlazeFace模型，明确地预测了两个额外的虚拟关键点，这些虚拟关键点将人体中心、旋转和缩放牢固地描述为一个圆圈，预测一个人臀部的中点，外接整个人的圆的半径，以及连接肩部和臀部中点的线的倾斜角。

利用两步检测器-跟踪器 ML 管道，使用检测器，管道首先在帧内定位人/姿势感兴趣区域 (ROI)。跟踪器随后裁剪帧ROI作为输入来预测 ROI 内的姿势标志和分割掩码。请注意，对于视频用例，仅在需要时调用检测器，即在第一帧以及当跟踪器无法再识别前一帧中存在的身体姿势时。对于其他帧，管道只是从前一帧的姿势地标中导出 ROI。

<img src="https://google.github.io/mediapipe/images/mobile/pose_tracking_detector_vitruvian_man.png" alt="pose_tracking_detector_vitruvian_man.png " style="zoom:150%;" />

从Mediapipe上获取的身体的33个特征点，具体如下图：

<img src="https://cimgioc.oss-cn-beijing.aliyuncs.com/pose_tracking_full_body_landmarks.png" alt="pose_tracking_full_body_landmarks.png " style="zoom:80%;" />

### 人体姿态识别

人机交互的识别过程中挑战性比较大之一就是人体姿态识别，主要解决：

1. 身份识别问题：图片分辨率太低导致人脸识别失效的情况；

2. 辅助验证问手段：配合人脸识别等识别方法提高识别准确率；

3. 图像检索等；

4. 其他。

人体姿态识别包括动作识别、身份识别两个方面，关键在人体特征提取，人体特征提取主要完成动作特征提取、身份特征提取。一般实现流程为：

#### 动作识别

![image-20220529105139605](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529105139605.png)

视频逐帧分析，采用连续的动作识别出人物动作，如走路、跑步、蹲下等。在计算机视觉中人机交互中有很大的应用，主要处理模型大概分为两个大类：卷积神经网络（3D-CNN）、基于循环神经网络与其扩展模型（CNN + LSTM）。

动作识别分析的应用场景有：3D 个人健身教练、3D试衣、游戏人物动作设计、古画分析、人体绘画教学、游戏的交互、照相馆拍照姿势指南等等。

动作分析效果如：

![image-20220529110328200](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529110328200.png)

#### 身份识别

![image-20220529105149895](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529105149895.png)

行人重识别、步态识别是身份识别的实现方法。

**行人重识别**（Person re-identification，简称ReID）也称行人再识别，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。广泛被认为是一个图像检索的子问题。旨在弥补固定的摄像头的视觉局限，并可与行人检测/行人跟踪技术相结合，可广泛应用于智能视频监控、智能安保等领域，行人重识别是生物识别的一个重要部分，可以作为人脸识别的一个补充。

**步态识别**，首先对视频预处理将行人与背景分离，形成黑白轮廓图silhouette。然后再在连续多帧的silhouette图中获取特征，最终达到身份识别的目的，依据是每个人都会有自己的走路方式，这是一种比较复杂的行为特征。

silhouette图如下（来自CASIA-B）：

![image-20220529110350798](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529110350798.png)

在监控视频中，由于相机分辨率和拍摄角度的缘故，通常无法得到质量非常高的人脸图片。当人脸识别失效的情况下，ReID、步态识别就成为了一个非常重要的替代技术。ReID有一个非常重要的特性就是跨摄像头，所以学术论文里评价性能的时候，是要检索出不同摄像头下的相同行人图片。ReID、步态识别已经在学术界研究多年，但直到最近几年随着深度学习的发展，才取得了巨大的突破。

## 人体姿态识别在健身中的应用

人体姿态识别是计算机视觉中的重要任务，也是计算机理解人体的动作、行为不可或缺的一部分。它的功能特点可以应用到体育健身，根据人体关键点信息，分析人体姿态、运动轨迹、动作角度等，辅助运动员进行体育训练，分析健身锻炼效果，提升教学效率。在这些应用中对人体姿态的判定是一个关键，通过姿态判定进而判断运动员或者体育爱好者的动作是否符合要求，对于体操，瑜伽等对姿态要求比较高的体育活动；甚至是体感游戏都有很好的帮助。

在体育健身场景中，人体姿态估计算法，除了能够识别各种动作及对风险动作进行识别预警，进而还能给出动作准确度等信息反馈，利用运动更精准、更实时、更多人的准确性判断，同时也可对跳绳、深蹲、俯卧撑等各类动作进行运动计数等更多数字化体育科技服务。

对于AI健身教练应用程序，常见流程如下：

1. 在做练习时捕捉用户的动作
2. 分析运动成绩的正确性
3. 在用户界面显示错误

致力于AI 运动算法的GoMore公司曾推出两款AI教练策略型产品，一个是针对卡路里燃烧需求的AI卡路里教练；另一个则是唤醒健康生活的AI活力教练，两款应用产品各有侧重。

- AI卡路里教练则是对健身目标明确的用户，用个人化训练、课程指导、运动分析三个步骤，来帮助用户达到理想且科学化的训练效果。其中每个步骤都有会适当体能训练的状态监控，再通过训练效果来分析运动成效，给予相应建议。
- AI活力教练是基于当下生活人群压力大、睡眠不足、懒惰这类亚健康状态人群而设置，从个人评估、目标设定、实时引导、状态分析、生活平衡的循环指导，帮助用户唤醒生活活力。

虽然说，技术的日益进步，人工智能在生活无处不在，但是无论传统线下教练还是AI智能教练，都已经各有优势，AI智能教练的数据精准性已经给科学的锻炼计划提供了更大帮助与支持，同时健身教练的丰富一线从业经验则能给用户提供情感和力量支持，二者更多是互相包容的关系。

新冠疫情之后，健身行业充满不确定性，行业内产品出现更迭变化，在线直播课程、新兴物联网健身器材、运动成效分析层出不穷，倘若预测未来健身行业发展趋势，那么定制化、数据化的健身模型将是一个流行话题，如果能够真正帮助不同健身需求的用户提高运动效率，那么AI智能教练才能够真正融入生活，要达到这个目标，需要全行业的共同努力。

# 设计实现

使用MediaPipe Pose实现人体姿态识别，即骨骼关键点检测，输出为具有“pose_landmarks”字段的 NamedTuple 对象，其中包含检测到的最突出人物的姿势标志。

使用opencv提取每一帧，经过Mediapipe的pose解决方案检测，将输出绘制在视频帧上即可实时显示检测到的关键点。

通过手脚、手膝盖间的距离小于阈值来近似代替接触，从而提取关键帧。

在关键帧中使用检测到的关键点坐标之间的角度或距离分析动作细节是否到位，从而给出健身动作指导；

## 人体骨骼关键点检测

调用MediaPipe Pose，初始化pose检测器

```
self.mpDraw = mp.solutions.drawing_utils
self.mpPose = mp.solutions.pose
self.pose = self.mpPose.Pose(self.mode, self.complexity, self.smooth_landmarks, 
    self.enable_segmentation, self.smooth_segmentation, 
    self.detectionCon, self.trackCon)
```

检测骨骼关键点

```
imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 色彩转换（imread函数读入的是BGR格式）
self.pose_results = self.pose.process(imgRGB) # 姿态识别
```

获取骨骼关键点坐标

```
self.lmList = []
if self.pose_results.pose_landmarks:
    for id, lm in enumerate(self.pose_results.pose_landmarks.landmark):
        h, w, c = img.shape                    # 缩放后的坐标
        cx, cy = int(lm.x * w), int(lm.y * h)  # 真实坐标
        self.lmList.append([id, cx, cy])
```

经过上述步骤检测的结果如下图：

<img src="https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220529114245315.png" alt="image-20220529114245315 " style="zoom:33%;" />

## 动作要点检测与指导

依据刘教练直播时的动作要点讲解，总结出本草纲目毽子舞中各个动作的要点（第四个动作由于技术限制未实现）：

- 动作一 盘踢：髋关节要打开，大腿与小腿夹角应小于90°
- 动作二 磕踢：大腿抬起，达到水平，膝盖和腰的高度应保持在同一高度
- 动作三 摸脚踢：脚尽量向身体侧面踢，距离身体足够远

这里讲解的都是最重要的动作要点，前提假设用户已经熟悉并能够做到基本的动作要领，包括做动作的时候手脚要碰到，或手和膝盖要碰到，这是成功提取关键帧的前提。

实现思路：对于每一个动作的检测原理基本相同，分为两步：

- 提取关键帧：依据骨骼点之间的距离确定动作关键帧
- 动作指导：依据关键帧中骨骼点角度或距离是否符合规定范围来判断动作是否合格，如果不合格，给出指导

### 动作一

进行判断的前提（提取关键帧的依据）是：手脚相碰，即距离小于阈值

```
lh_lf_dis = self.findDis(19, 27)
rh_rf_dis = self.findDis(20, 28)
h_f_thr = 1000  # 设定阈值
if lh_rf_dis < h_f_thr:
	pass
elif rh_lf_dis < h_f_thr:
	pass
```

判断动作是否合格：要求腿弯曲到90°以内，即在关键帧中，骨骼点24, 26, 28（右腿）或23, 25, 27（左腿）的夹角应小于90°

![image-20220614094516261](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614094516261.png)

以右盘腿踢为例的代码实现：

```
# 计算大腿小腿夹角
angle_r = self.findAngle(img, 24, 26, 28)
if angle_r > 90:
	# 如果动作不标准，画出来提醒用户
	self.drawAngle(img, 24, 26, 28)
	# 给出指导意见
	cv2.putText(img, str('open your hip joint'), (70, 50),
                    cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)
	# 语音提醒用户
	pyttsx3.speak("髋关节要打开")
```

动作指导效果：

![image-20220614095720696](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614095720696.png)

### 动作二

进行判断的前提（提取关键帧的依据）是：手、膝盖相碰，即距离小于阈值

```
lh_rk_dis = self.findDis(15, 26)
rh_lk_dis = self.findDis(16, 25)
h_k_thr = 1000  # 设定阈值
if lh_rk_dis < h_k_thr:
	pass
elif rh_lk_dis < h_k_thr:
	pass
```

判断动作是否合格：要求大腿水平，通过膝盖和腰的纵坐标来度量，即在关键帧中，骨骼点24, 26（右腿）或23, 25（左腿）的纵坐标应相近。

检测此动作时，应侧对摄像头以判断髋关节和膝盖的位置。

![image-20220614111341622](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614111341622.png)

以右磕踢为例的代码实现：

```
# 设定高度误差范围
dis_thr_down = -10
dis_thr_up = 30
# 计算膝盖与髋关节的高度差
dis = self.lmList[24][2] - self.lmList[26][2]
if dis < dis_thr_down or dis > dis_thr_up:
	# 如果动作不标准，画出来提醒用户
	cv2.circle(img, (self.lmList[24][1], self.lmList[24][2]), 15, (0, 0, 255), cv2.FILLED)
	cv2.circle(img, (self.lmList[26][1], self.lmList[26][2]), 15, (0, 0, 255), cv2.FILLED)
	# 给出指导意见
	cv2.putText(img, str('legs not horizontal'), (70, 50),
                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)
	# 语音提醒用户
	pyttsx3.speak("大腿要伸平")
```

动作指导效果：

![image-20220614111826833](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614111826833.png)

### 动作三

进行判断的前提（提取关键帧的依据）是：手脚相碰，即距离小于阈值

```
lh_lf_dis = self.findDis(19, 27)
rh_rf_dis = self.findDis(20, 28)
h_f_thr = 1000  # 设定阈值
if lh_lf_dis < h_f_thr:
	pass
elif rh_rf_dis < h_f_thr:
	pass
```

判断动作是否合格：要求手脚距离身体足够远，通过手和头的横坐标来度量，即在关键帧中，骨骼点0, 16（右腿）或0, 15（左腿）的横坐标应大于设定值。

![image-20220614112202932](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614112202932.png)

以左摸腿踢为例的代码实现：

```
# 设定距离阈值
dis_thr = 80
# 计算头与手的距离
dis = self.lmList[15][1] - self.lmList[0][1]
if dis < dis_thr:
	# 如果动作不标准，画出来提醒用户
	cv2.circle(img, (self.lmList[15][1], self.lmList[15][2]), 15, (0, 0, 255), cv2.FILLED)
	# 给出指导意见
	cv2.putText(img, str('stretch legs out'), (70, 50),
                        cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)
	# 语音提醒用户
	pyttsx3.speak("腿要向外伸展")
```

动作指导效果：

![image-20220614142828269](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220614142828269.png)

# 运行说明

文件结构：

- main.py：包含参数选择、视频流处理等。
- coach.py：Coach类的具体实现，包括提取骨骼关键点、计算距离角度、判断动作是否标准并给与提示等。

使用时，先在main.py line 16选择动作种类，然后运行即可，程序将会自动打开摄像头读取视频并处理。

注意在运行时，第一、三个动作应做到手与脚相碰，第二个动作应侧对摄像头并做到手与膝盖相碰。

# 问题与解决方法

- 初始化pose时，通过官方文档介绍设置参数，但是出现错误

  ```
  self.pose = self.mpPose.Pose(self.mode, self.complexity, self.smooth_landmarks, self.enable_segmentation, 
                   self.smooth_segmentation, self.refine_face_landmarks, self.detectionCon, self.trackCon)
  ```

  ![image-20220609095758499](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220609095758499.png)

  经过查看pose源码发现，参数项没有`refine_face_landmarks`，将其删掉即可。

  ![image-20220609100055257](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220609100055257.png)

  本来以为是文档写错了，后来发现是自己看错文档了，所以写错了API参数。

  ![image-20220609100526904](https://cimgioc.oss-cn-beijing.aliyuncs.com/image-20220609100526904.png)

# 总结

创新点：

- 基于骨架的模型从图像中检测和分析人体关节的X，Y坐标
- 通过手脚、手膝盖间的距离小于阈值来近似代替接触，从而提取关键帧
- 使用检测到的关键点坐标之间的角度或距离分析动作细节是否到位
- 在界面上标记提示并语音播报指导意见

不足之处：

- 检测健身运动中过程中存在一定的运动误差
- 对于使用的二维关键点检测模型，遮挡或快速移动的关节可能会导致错误/随机检测
- 当前方案只针对特定模型，特定动作规范，内容不够完善充实，不具有广泛性。
